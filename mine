# -*- coding: utf-8 -*-
"""Moringa_Data_Science_Core_W2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EyjBWpU-w1v5a7vmG9CGljUbzobcdBdm

Importing the necessary libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""Importing the Financial Dataset"""

df=pd.read_csv('Financial Dataset.csv',delimiter=',')

"""Previeweing the dataset"""

df.head(2)

"""Finding more information about the dataset"""

df.info()

df.rename(columns={'Level of Educuation' : 'Level of Education', 'The relathip with head' : "The relationship with head"},inplace=True)

"""# The Data Cleaning Process

Standardizing the column names.
"""

df.columns=map(str.lower,df.columns)

"""Removing the space between letters and replacing with a _ sign."""

df.columns=df.columns.str.replace(" ","_")

"""Checking for outliers in the dataset and we find out that there are inappropriate entries in the year column. I assume that it was an mistake in the entry in our dataset and shall not be removed."""

outliers=[]
def detect_outlier(data):
  threshold=3
  mean_1=np.mean(data)
  std_1=np.std(data)

  for y in data:
    z_score=(y-mean_1)/std_1
    if np.abs(z_score)>threshold:
      outliers.append(y)
  return outliers
detect_outlier(df['year'])

"""Checking for null values in our dataset"""

df.isnull().sum()

"""Dropping the null values and fining out the shape of our new dataframe"""

df=df.dropna()
df.shape

"""# The Univariate Analysis

Finding out the representation of countries in our dataset. Rwanda has the highest number of respondents while Uganda has the lowest number of respondents.
"""

df.country.value_counts()

"""Visualizing the information above in a bar graph"""

df['respondent_age'].mean()

df['respondent_age'].mode()

df['respondent_age'].median()

df['respondent_age'].skew()

df['respondent_age'].std()

df['respondent_age'].var()

df.kurt()

df.describe()

df['country'].value_counts().plot.bar()
plt.xlabel('Country')
plt.ylabel('Number Of participants')
plt.title('Bar graph representing the distribution of participants across East Africa')
plt.show()

"""Finding out the location that had the highest participants. Where I found out the rural area had the highest number."""

df['type_of_location'].value_counts().plot.bar()
plt.xlabel('Type Of Location')
plt.ylabel('Number Of Participants')
plt.title('Bar graph of Location against Participants')
plt.show()

bi=df.groupby(['country'])['household_size'].sum()
bi.plot.bar()
plt.xlabel('Country')
plt.ylabel('The Number Of Persons')
plt.title('Number of Persons in a Country')
plt.show()

"""Finding out whether household size has an impact on having a bank account. In the graph below shows there is no significant difference."""

new=df.groupby(['has_a_bank_account'])['household_size'].median()
new.plot.bar()
plt.xlabel('Bank account decision')
plt.ylabel('Average household size')
plt.title('Graph of bank account vs household size')

"""Finding out the country with the highest number of bank holders. Kenya has the highest number of bank holders while Uganda has the lowest number of bank holders."""

df1=df[df['has_a_bank_account']=='Yes']
df2=df1.groupby(['country'])['has_a_bank_account'].count()
df2.plot.bar()
plt.xlabel('Country')
plt.ylabel('Number of respondents with a bank account')
plt.title('The number of respondents per country with a bank account')
plt.show()

"""In the graph below we finding out whether the type of location has an effect on whether one has a bank account, which I found out does not affect."""

df3=df1.groupby(['type_of_location'])['has_a_bank_account'].count()
df3.plot.bar()
plt.xlabel('The type of Location')
plt.ylabel('The number of respondents with a bank account')
plt.title('The number of respondents in a location with a bank account')
plt.show()

df4=df1.groupby(['level_of_education'])['has_a_bank_account'].count()
df4.plot.bar()
plt.xlabel('The level of education')
plt.ylabel('The number of respondents with bank accounts')
plt.title('The number of respondents per level of education')
plt.show()

df5=df1.groupby(['marital_status'])['has_a_bank_account'].count()
df5.plot.bar()
plt.xlabel('The marital status')
plt.ylabel('The number of respondents with bank accounts')
plt.title('The number of respondents per marital status')
plt.show()

df6=df1.groupby(['cell_phone_access'])['has_a_bank_account'].count()
df6.plot.pie()
plt.title("Pie chart for number of people who have bank accounts and cell phone access")
plt.show()

df7=df1.groupby(['gender_of_respondent'])['has_a_bank_account'].count()
df7.plot.pie()
plt.title("Pie chart for number of people who have bank accounts and cell phone access")
plt.show()

"""# The Bivariate Analysis"""

!pip install -q researchpy
import researchpy as rp
rp.summary_cont(df.groupby('country')['respondent_age'])

encode={"Has a Bank account" : {"Yes" : 1, "No": 0},"Type of Location" : {"Rural" : 1, "Urban" : 0},"Cell Phone Access" : {"Yes" : 1, "No" : 0}, "gender_of_respondent" : {"Female" : 1, "Male" : 1}}

df.replace(encode, inplace=True)
df.head()

df.info()

df.drop(['country','year','uniqueid','the_relationship_with_head','marital_status','level_of_education','type_of_job'],axis=1,inplace=True)

"""# Principal Component Analysis

Defining the variables to work with in our dimension reduction using Principal Component Analysis.
"""

X=df.drop('Has a Bank account',1)
y=df['Has a Bank account']

"""Splitting the dataset into the test and train and defining the size of the test data which will be 20% of the data."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=0)

"""Importing the Standard Scaler from Sklearn to normalize the distribution of the data into normal distribution."""

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

"""Importing the PCA for our analysis which will only be used in the train data and all the factors will e considered."""

from sklearn.decomposition import PCA

pca=PCA()

X_train=pca.fit_transform(X_train)
X_test=pca.transform(X_test)

"""Finding out the variance for each principal component of our analysis. In our analysis we find out that the first four components account for 100% of the classification of whether one can pick a bank account or not. The first three components account for 82.9%"""

explained_variance=pca.explained_variance_ratio_
explained_variance

"""We are going to use the first component for our analysis in our prediction model."""

from sklearn.decomposition import PCA
pca=PCA(n_components=1)

X_train=pca.fit_transform(X_train)
X_test=pca.transform(X_test)

"""Here we are going to train the model using the Random Forest Classifier to help out in the classification."""

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(max_depth=0.2, random_state=0)
classifier.fit(X_train, y_train)

"""Here we are going to test using the data that was set apart for the test and train to find out the efficiency of the model."""

y_pred=classifier.predict(X_test)

"""Importing the necessary libraries for measring the accuracy of the model."""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

"""We find out the first principal Component has an 85% predictive capacity."""

cm=confusion_matrix(y_test,y_pred)
print(cm)
print('Accuracy', accuracy_score(y_test,y_pred))

"""Testing the model for factor analysis."""

from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity

chi_square_value, p_value=calculate_bartlett_sphericity(df)
chi_square_value, p_value

"""We find our dataset has no capacity for factor analysis with nan KMO value."""

#!pip install factor_analyzer==0.2.3
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all, kmo_model=calculate_kmo(df)
kmo_all, kmo_model

"""# Linear Discriminant Analysis

I divided the dataset.
"""

x=df.iloc[:,1:6].values
Y=df.iloc[:,0].values

"""Importing the train_test_split library to split the data into train and test."""

from sklearn.model_selection import train_test_split

x_train, x_test, Y_train, Y_test=train_test_split(x,Y,test_size=0.2,random_state=0)

"""Creating a normal distribution of the data."""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

"""Importing the Linear Discriminant Analysis and using the first two components in our analysis."""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda=LDA(n_components=2)
x_train=lda.fit_transform(x_train,Y_train)
x_test=lda.transform(x_test)

"""Using the Random Forest Classifier in our classification model."""

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(max_depth=0.2, random_state=0)
classifier.fit(x_train, Y_train)
Y_pred=classifier.predict(x_test)

"""Finding out the predictive capacity of our Linear Discriminant model."""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm=confusion_matrix(Y_test,Y_pred)
print(cm)
print('Accuracy' + str(accuracy_score(Y_test,Y_pred)))

"""Defining the metric for success.

1. Which country is likely to have a new bank account holder? A person living in Kenya is more likely to open a new bank account.

2. Which location is likely to have a new bank account holder? There is no much difference of a person living in the rural area and urban area.

3. What Gender is likely to have a new bank account? A male is more likely to open a bank account more than a female. Though there is no much significant difference.

4.Which marital status is more likely to have a new bank account? A married person is more likely to open a new bank account.

5. What level of education is more likely to have new bank account? A person with primary level education is more likely to open a new bank account.

Our main experimental question will be to find out whether a married female adult living in the rural area, has cell phone access and is married is likely to have a bank account? Given the weighty married,and cell phone access, it is more likely the individual will open a new bank account.

Explorative Data Analysis.

The data was enough for my analysis. The data was right for the survey and four components as proved in the Principal Component Analysis and Linear Discriminant Analysis were enough for the analysis.
"""
